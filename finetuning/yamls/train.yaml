model:
  model_name_or_path: 
  use_auth_token: True

data:
  dataset_name: yale-nlp/QTSumm
  preprocess_func: preprocess_qtsumm_function
  max_source_length: 1024
  max_target_length: 256
  val_max_target_length: 256
  max_train_samples: null
  max_eval_samples: null
  num_beams: 5
  ignore_pad_token_for_loss: True
  inference_set: validation

training:
  do_train: True
  output_dir: outputs/finetuning_outputs/
  overwrite_output_dir: True
  per_device_train_batch_size: 8 # this setting fits for 48 GB GPU
  gradient_accumulation_steps: 8
  per_device_eval_batch_size: 2
  predict_with_generate: True
  num_train_epochs: 20
  warmup_ratio: 0.1 
  learning_rate: 0.00002 
  fp16 : True
  logging_steps: 10 
  eval_steps: 100 
  save_steps: 100
  evaluation_strategy: steps 
  weight_decay: 0.01 
  label_smoothing_factor: 0.1  
  save_total_limit: 8 
  report_to: wandb