[
    {
        "model_name": "gpt-4",
        "sacreBLEU": 18.96138288148023,
        "Rouge-L": 0.3993109948920145,
        "METEOR": 0.5124668006209476,
        "BERTScore": 0.909913103633555,
        "TAPAS": 0.9434137344360352,
        "AutoACU": 0.600658552047273,
        "Prediction Length": 92.10667903525047
    },
    {
        "model_name": "gpt-3.5-turbo",
        "sacreBLEU": 20.1593228776314,
        "Rouge-L": 0.40009796331050085,
        "METEOR": 0.4972974706332032,
        "BERTScore": 0.9094185390698003,
        "TAPAS": 0.9174396991729736,
        "AutoACU": 0.5562274026613621,
        "Prediction Length": 87.99907235621521
    },
    {
        "model_name": "llama-2-70b-chat-awq",
        "sacreBLEU": 18.195937660960436,
        "Rouge-L": 0.3729281208093145,
        "METEOR": 0.46167114658407193,
        "BERTScore": 0.9022869801698236,
        "TAPAS": 0.8645640015602112,
        "AutoACU": 0.48083055858951274,
        "Prediction Length": 92.80426716141002
    },
    {
        "model_name": "vicuna-33b-v1.3",
        "sacreBLEU": 19.335595588475204,
        "Rouge-L": 0.37043391381786184,
        "METEOR": 0.43841824823676,
        "BERTScore": 0.9013069884675332,
        "TAPAS": 0.7838590145111084,
        "AutoACU": 0.45279526704981066,
        "Prediction Length": 75.02504638218925
    },
    {
        "model_name": "lemur-70b-chat-v1-awq",
        "sacreBLEU": 14.336823406320251,
        "Rouge-L": 0.31539620659339823,
        "METEOR": 0.3827476647389684,
        "BERTScore": 0.8809701772700436,
        "TAPAS": 0.8126159906387329,
        "AutoACU": 0.3983725877150423,
        "Prediction Length": 86.51669758812616
    },
    {
        "model_name": "llama-2-7b-chat-hf",
        "sacreBLEU": 13.625967650893521,
        "Rouge-L": 0.32279651541158655,
        "METEOR": 0.42542159004563973,
        "BERTScore": 0.8905089782319396,
        "TAPAS": 0.7532467842102051,
        "AutoACU": 0.3850950699555299,
        "Prediction Length": 104.99257884972171
    },
    {
        "model_name": "mistral-7b-instruct-v0.1",
        "sacreBLEU": 13.689808695122148,
        "Rouge-L": 0.3145507064692902,
        "METEOR": 0.40894678444876825,
        "BERTScore": 0.8889117011997386,
        "TAPAS": 0.7170686721801758,
        "AutoACU": 0.36704008917778674,
        "Prediction Length": 96.56493506493507
    },
    {
        "model_name": "llama-2-13b-chat-hf",
        "sacreBLEU": 13.817983498681352,
        "Rouge-L": 0.2382947320012069,
        "METEOR": 0.28088608823809935,
        "BERTScore": 0.866309521978994,
        "TAPAS": 0.8135436177253723,
        "AutoACU": 0.2653836156975941,
        "Prediction Length": 61.473098330241186
    }
]