[
    {
        "model_name": "gpt-4",
        "sacreBLEU": 19.761471216546777,
        "Rouge-L": 0.38426519056367153,
        "METEOR": 0.4837088492580398,
        "BERTScore": 0.8576981985392066,
        "TAPAS": 0.9230055809020996,
        "AutoACU": 0.5755170516268884,
        "Prediction Length": 86.85621521335807
    },
    {
        "model_name": "gpt-3.5-turbo",
        "sacreBLEU": 21.10164965053839,
        "Rouge-L": 0.40760472324868546,
        "METEOR": 0.49146376956562215,
        "BERTScore": 0.9106359032288553,
        "TAPAS": 0.8970315456390381,
        "AutoACU": 0.5549341977154059,
        "Prediction Length": 82.5139146567718
    },
    {
        "model_name": "llama-2-70b-chat-awq",
        "sacreBLEU": 17.21499788655525,
        "Rouge-L": 0.35233561626412674,
        "METEOR": 0.4407832367533628,
        "BERTScore": 0.8975957202446925,
        "TAPAS": 0.856215238571167,
        "AutoACU": 0.4572578660352415,
        "Prediction Length": 91.5213358070501
    },
    {
        "model_name": "lemur-70b-chat-v1-awq",
        "sacreBLEU": 13.290910757081905,
        "Rouge-L": 0.3087048828369511,
        "METEOR": 0.3991693030292594,
        "BERTScore": 0.8784320496561797,
        "TAPAS": 0.8283858895301819,
        "AutoACU": 0.4078522929990861,
        "Prediction Length": 102.25881261595548
    },
    {
        "model_name": "vicuna-33b-v1.3",
        "sacreBLEU": 15.050865233082778,
        "Rouge-L": 0.3261212417735006,
        "METEOR": 0.4215264331432087,
        "BERTScore": 0.8921661538958329,
        "TAPAS": 0.8200371265411377,
        "AutoACU": 0.3999398958926298,
        "Prediction Length": 93.81447124304268
    },
    {
        "model_name": "mistral-7b-instruct-v0.1",
        "sacreBLEU": 13.750763576380065,
        "Rouge-L": 0.3171340697582218,
        "METEOR": 0.41351014971544786,
        "BERTScore": 0.890697564618707,
        "TAPAS": 0.7291280031204224,
        "AutoACU": 0.3751633514327054,
        "Prediction Length": 98.39888682745826
    },
    {
        "model_name": "llama-2-7b-chat-hf",
        "sacreBLEU": 13.260599764844073,
        "Rouge-L": 0.31300624254442605,
        "METEOR": 0.4252293243233714,
        "BERTScore": 0.8882811890019999,
        "TAPAS": 0.7810760736465454,
        "AutoACU": 0.37297880644383036,
        "Prediction Length": 110.32745825602969
    },
    {
        "model_name": "llama-2-13b-chat-hf",
        "sacreBLEU": 14.642347152955313,
        "Rouge-L": 0.25459716816845523,
        "METEOR": 0.3091841983950507,
        "BERTScore": 0.8675365821771144,
        "TAPAS": 0.7662338018417358,
        "AutoACU": 0.28643207334272275,
        "Prediction Length": 64.33673469387755
    }
]